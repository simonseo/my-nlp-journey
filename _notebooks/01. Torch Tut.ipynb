{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b95afbd",
   "metadata": {},
   "source": [
    "- [PyTorch for Deep Learning - Full Course (FreeCodeCamp Video)](https://www.youtube.com/watch?v=GIsg-ZUy0MY&t=205s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e2d722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ec2f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4951bd9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cb351f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9009fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119cc7f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af64a0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da73bbd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3],\n",
       "        [ 5, 43,  2]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[1,2,3], [5,43,2]]\n",
    "x_data  = torch.tensor(data)\n",
    "x_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b8d8b55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1,  2,  3],\n",
       "        [ 5, 43,  2]]),\n",
       " tensor([[ 1,  2,  3],\n",
       "         [ 5, 43,  2]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "np_array, x_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b349f3",
   "metadata": {},
   "source": [
    "### tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "362527ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(3.)\n",
    "w = torch.tensor(4., requires_grad=True)\n",
    "b = torch.tensor(5., requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9558e62f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17., grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x*w +b\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f54166e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cbcd9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None tensor(3.) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/torchtest/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print(x.grad, y.grad, w.grad, b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58845b85",
   "metadata": {},
   "source": [
    "### numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7108eb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "498cad44",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Required argument 'object' (pos 1) not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2e51aa4d54bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: Required argument 'object' (pos 1) not found"
     ]
    }
   ],
   "source": [
    "x = np.array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edac3fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, <function numpy.array>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ndarray, np.array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9662a0",
   "metadata": {},
   "source": [
    "### apples and oranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c8b23fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe3effdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = (temp, rain, humidity)\n",
    "inputs = np.array([[73,67,43],\n",
    "     [91,88,64],\n",
    "     [102,43,37],\n",
    "     [87,134,58],\n",
    "     [69,96,70]], dtype='float32')\n",
    "inputs = torch.from_numpy(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f29fa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y = (apples, oranges)\n",
    "outputs = np.array([\n",
    "     [56,70],\n",
    "     [81,101],\n",
    "     [119,133],\n",
    "     [22,37],\n",
    "     [103,119]\n",
    " ], dtype='float32')\n",
    "outputs = torch.from_numpy(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "749aba2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9792, -0.1531, -0.2063],\n",
      "        [ 0.5462, -0.4614, -1.7207]], requires_grad=True) tensor([0.9383, 1.0918], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "w = torch.randn(2,3,requires_grad=True) # 2 output vars from 3 input vars.. after we transpose it\n",
    "b = torch.randn(2, requires_grad=True) # initialized with normal distribution with mean 0 and standard deviation 1.\n",
    "print(w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "045d2f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define mathetmatical model\n",
    "def model(x, w, b):\n",
    "    return x @ w.t() + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fb80deef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  53.2944,  -63.9394],\n",
      "        [  63.3736,  -99.9316],\n",
      "        [  86.6036,  -26.7034],\n",
      "        [  53.6525, -113.0154],\n",
      "        [  39.3684, -125.9624]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# see what it predicts\n",
    "preds = model(inputs)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc6d274c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "# see what it should predict\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f3fe4c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how different it is\n",
    "def mse(t1, t2):\n",
    "    diff = t1-t2\n",
    "    return torch.sum(diff*diff)/diff.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b2ac2b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(17274.8008, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = mse(preds, outputs)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a3d80a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w.grad before backpropagation: None\n",
      "tensor([[ 0.9792, -0.1531, -0.2063],\n",
      "        [ 0.5462, -0.4614, -1.7207]], requires_grad=True)\n",
      "w.grad after backpropagation: tensor([[ -1348.5496,   -998.5267,  -1012.2916],\n",
      "        [-14861.1689, -15428.3232, -10075.2607]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"w.grad before backpropagation: {w.grad}\")\n",
    "loss.backward()\n",
    "print(w)\n",
    "print(f\"w.grad after backpropagation: {w.grad}\") # df/dw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "45228942",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b.grad before backpropagation: tensor([ -16.9415, -177.9104])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling .backward() or autograd.grad() the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-6b960b0c44e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"b.grad before backpropagation: {b.grad}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"b.grad after backpropagation: {b.grad}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# df/db\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/torchtest/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/torchtest/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling .backward() or autograd.grad() the first time."
     ]
    }
   ],
   "source": [
    "print(f\"b.grad before backpropagation: {b.grad}\")\n",
    "loss.backward()\n",
    "print(b)\n",
    "print(f\"b.grad after backpropagation: {b.grad}\") # df/db\n",
    "\n",
    "\n",
    "# RuntimeError: Trying to backward through the graph a second time, \n",
    "# but the saved intermediate results have already been freed. \n",
    "# Specify retain_graph=True when calling .backward() or autograd.grad() the first time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b59380fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]]) tensor([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "w.grad.zero_()\n",
    "b.grad.zero_()\n",
    "print(w.grad, b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "688c36e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.randn(2,3,requires_grad=True) # 2 output vars from 3 input vars.. after we transpose it\n",
    "b = torch.randn(2, requires_grad=True) # initialized with normal distribution with mean 0 and standard deviation 1.\n",
    "preds = model(inputs, w, b)\n",
    "loss = mse(preds, outputs)\n",
    "loss.backward()\n",
    "\n",
    "# We use torch.no_grad to indicate to PyTorch \n",
    "# that we shouldn't track, calculate or modify gradients \n",
    "# while updating the weights and biases.\n",
    "with torch.no_grad():\n",
    "    learning_rate = 1e-5\n",
    "    w -= w.grad * learning_rate\n",
    "    b -= b.grad * learning_rate\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9366b67a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.2837,  1.9113, -0.8235],\n",
       "         [ 0.4683,  0.4118, -0.0504]], requires_grad=True),\n",
       " tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.]]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1041c4ba",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(166.0341, grad_fn=<DivBackward0>)\n",
      "tensor(165.9020, grad_fn=<DivBackward0>)\n",
      "tensor(165.7747, grad_fn=<DivBackward0>)\n",
      "tensor(165.6521, grad_fn=<DivBackward0>)\n",
      "tensor(165.5339, grad_fn=<DivBackward0>)\n",
      "tensor(165.4202, grad_fn=<DivBackward0>)\n",
      "tensor(165.3106, grad_fn=<DivBackward0>)\n",
      "tensor(165.2050, grad_fn=<DivBackward0>)\n",
      "tensor(165.1034, grad_fn=<DivBackward0>)\n",
      "tensor(165.0053, grad_fn=<DivBackward0>)\n",
      "tensor(164.9109, grad_fn=<DivBackward0>)\n",
      "tensor(164.8200, grad_fn=<DivBackward0>)\n",
      "tensor(164.7323, grad_fn=<DivBackward0>)\n",
      "tensor(164.6480, grad_fn=<DivBackward0>)\n",
      "tensor(164.5667, grad_fn=<DivBackward0>)\n",
      "tensor(164.4884, grad_fn=<DivBackward0>)\n",
      "tensor(164.4130, grad_fn=<DivBackward0>)\n",
      "tensor(164.3403, grad_fn=<DivBackward0>)\n",
      "tensor(164.2702, grad_fn=<DivBackward0>)\n",
      "tensor(164.2027, grad_fn=<DivBackward0>)\n",
      "tensor(164.1379, grad_fn=<DivBackward0>)\n",
      "tensor(164.0752, grad_fn=<DivBackward0>)\n",
      "tensor(164.0150, grad_fn=<DivBackward0>)\n",
      "tensor(163.9569, grad_fn=<DivBackward0>)\n",
      "tensor(163.9009, grad_fn=<DivBackward0>)\n",
      "tensor(163.8470, grad_fn=<DivBackward0>)\n",
      "tensor(163.7951, grad_fn=<DivBackward0>)\n",
      "tensor(163.7451, grad_fn=<DivBackward0>)\n",
      "tensor(163.6969, grad_fn=<DivBackward0>)\n",
      "tensor(163.6505, grad_fn=<DivBackward0>)\n",
      "tensor(163.6057, grad_fn=<DivBackward0>)\n",
      "tensor(163.5626, grad_fn=<DivBackward0>)\n",
      "tensor(163.5213, grad_fn=<DivBackward0>)\n",
      "tensor(163.4812, grad_fn=<DivBackward0>)\n",
      "tensor(163.4427, grad_fn=<DivBackward0>)\n",
      "tensor(163.4056, grad_fn=<DivBackward0>)\n",
      "tensor(163.3698, grad_fn=<DivBackward0>)\n",
      "tensor(163.3354, grad_fn=<DivBackward0>)\n",
      "tensor(163.3022, grad_fn=<DivBackward0>)\n",
      "tensor(163.2703, grad_fn=<DivBackward0>)\n",
      "tensor(163.2395, grad_fn=<DivBackward0>)\n",
      "tensor(163.2099, grad_fn=<DivBackward0>)\n",
      "tensor(163.1814, grad_fn=<DivBackward0>)\n",
      "tensor(163.1538, grad_fn=<DivBackward0>)\n",
      "tensor(163.1273, grad_fn=<DivBackward0>)\n",
      "tensor(163.1017, grad_fn=<DivBackward0>)\n",
      "tensor(163.0771, grad_fn=<DivBackward0>)\n",
      "tensor(163.0534, grad_fn=<DivBackward0>)\n",
      "tensor(163.0305, grad_fn=<DivBackward0>)\n",
      "tensor(163.0086, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    preds = model(inputs, w, b)\n",
    "    loss = mse(preds, outputs)\n",
    "    if (i % 20 == 0):\n",
    "        print(loss)\n",
    "    loss.backward()\n",
    "\n",
    "    # We use torch.no_grad to indicate to PyTorch \n",
    "    # that we shouldn't track, calculate or modify gradients \n",
    "    # while updating the weights and biases.\n",
    "    with torch.no_grad():\n",
    "        learning_rate = 1e-5\n",
    "        w -= w.grad * learning_rate\n",
    "        b -= b.grad * learning_rate\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fa116c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7763, -1.3736,  2.3343],\n",
      "        [ 0.8697, -1.4122,  2.5698]], requires_grad=True) tensor([ 1.1411, -0.4500], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "41bb8248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 66.1565,  78.9261],\n",
      "        [100.3044, 118.8911],\n",
      "        [107.6321, 122.6227],\n",
      "        [ 20.0077,  35.0331],\n",
      "        [ 86.2410, 103.8777]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "baaf1de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e66338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12fa10af",
   "metadata": {},
   "source": [
    "### Linear Regression using torch.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "12d95bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# allows us to access a small section of the training data using the array indexing notation ([0:3] in the above code). It returns a tuple (or pair), in which the first element contains the input variables for the selected rows, and the second contains the targets.\n",
    "from torch.utils.data import TensorDataset \n",
    "\n",
    "#  split the data into batches of a predefined size while training\n",
    "from torch.utils.data import DataLoader \n",
    "\n",
    "# Import nn.functional - contains many useful loss functions and several other utilities.\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "127697f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input (temp, rainfall, humidity)\n",
    "inputs = np.array([[73, 67, 43], [91, 88, 64], [87, 134, 58], \n",
    "                   [102, 43, 37], [69, 96, 70], [73, 67, 43], \n",
    "                   [91, 88, 64], [87, 134, 58], [102, 43, 37], \n",
    "                   [69, 96, 70], [73, 67, 43], [91, 88, 64], \n",
    "                   [87, 134, 58], [102, 43, 37], [69, 96, 70]], \n",
    "                  dtype='float32')\n",
    "\n",
    "# Targets (apples, oranges)\n",
    "targets = np.array([[56, 70], [81, 101], [119, 133], \n",
    "                    [22, 37], [103, 119], [56, 70], \n",
    "                    [81, 101], [119, 133], [22, 37], \n",
    "                    [103, 119], [56, 70], [81, 101], \n",
    "                    [119, 133], [22, 37], [103, 119]], \n",
    "                   dtype='float32')\n",
    "\n",
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e1b1f272",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2d298ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 67.,  88., 134.]), tensor([ 70., 101., 133.]))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0:3, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c1609ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([15, 3]), 15)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.size(), len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fc396381",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "\n",
    "# shuffle set to True to have the data reshuffled at every epoch\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7f049e3d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[102.,  43.,  37.],\n",
      "        [ 69.,  96.,  70.],\n",
      "        [ 73.,  67.,  43.],\n",
      "        [ 91.,  88.,  64.],\n",
      "        [102.,  43.,  37.]])\n",
      "tensor([[ 22.,  37.],\n",
      "        [103., 119.],\n",
      "        [ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [ 22.,  37.]])\n",
      "tensor([[ 69.,  96.,  70.],\n",
      "        [ 91.,  88.,  64.],\n",
      "        [ 69.,  96.,  70.],\n",
      "        [102.,  43.,  37.],\n",
      "        [ 73.,  67.,  43.]])\n",
      "tensor([[103., 119.],\n",
      "        [ 81., 101.],\n",
      "        [103., 119.],\n",
      "        [ 22.,  37.],\n",
      "        [ 56.,  70.]])\n",
      "tensor([[ 87., 134.,  58.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [ 91.,  88.,  64.],\n",
      "        [ 73.,  67.,  43.]])\n",
      "tensor([[119., 133.],\n",
      "        [119., 133.],\n",
      "        [119., 133.],\n",
      "        [ 81., 101.],\n",
      "        [ 56.,  70.]])\n"
     ]
    }
   ],
   "source": [
    "# shuffled on every iteration\n",
    "for xbatch, ybatch in train_dl:\n",
    "    print(xbatch)\n",
    "    print(ybatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b82215d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(3,2) # 3 input vars to 2 output vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "57c81fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1638,  0.2068, -0.3615],\n",
      "        [ 0.5245, -0.1700,  0.0985]], requires_grad=True) Parameter containing:\n",
      "tensor([-0.4813, -0.3101], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.weight, model.bias)\n",
    "# y = xW^T + b\n",
    "# shape of W is 2x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "52bc261d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "32c3d5aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.7897, 30.8231],\n",
       "        [ 9.4902, 38.7623],\n",
       "        [20.5184, 28.2525],\n",
       "        [11.7449, 49.5232],\n",
       "        [ 5.3723, 26.4540],\n",
       "        [ 9.7897, 30.8231],\n",
       "        [ 9.4902, 38.7623],\n",
       "        [20.5184, 28.2525],\n",
       "        [11.7449, 49.5232],\n",
       "        [ 5.3723, 26.4540],\n",
       "        [ 9.7897, 30.8231],\n",
       "        [ 9.4902, 38.7623],\n",
       "        [20.5184, 28.2525],\n",
       "        [11.7449, 49.5232],\n",
       "        [ 5.3723, 26.4540]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d2500bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cosine_embedding_loss',\n",
       " 'ctc_loss',\n",
       " 'gaussian_nll_loss',\n",
       " 'hinge_embedding_loss',\n",
       " 'l1_loss',\n",
       " 'margin_ranking_loss',\n",
       " 'mse_loss',\n",
       " 'multi_margin_loss',\n",
       " 'multilabel_margin_loss',\n",
       " 'multilabel_soft_margin_loss',\n",
       " 'nll_loss',\n",
       " 'poisson_nll_loss',\n",
       " 'smooth_l1_loss',\n",
       " 'soft_margin_loss',\n",
       " 'triplet_margin_loss',\n",
       " 'triplet_margin_with_distance_loss']"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda x: 'loss' in x, dir(F)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d42bf20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5168.5991, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss = F.mse_loss(model(inputs), targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "cfb8d9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "37676e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to train the model\n",
    "def fit(num_epochs, model, loss_fn, opt, train_dl):\n",
    "    \n",
    "    # Repeat for given number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Train with batches of data\n",
    "        for xb,yb in train_dl:\n",
    "            \n",
    "            # 1. Generate predictions\n",
    "            pred = model(xb)\n",
    "            \n",
    "            # 2. Calculate loss\n",
    "            loss = loss_fn(pred, yb)\n",
    "            \n",
    "            # 3. Compute gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # 4. Update parameters using gradients\n",
    "            opt.step()\n",
    "            \n",
    "            # 5. Reset the gradients to zero\n",
    "            opt.zero_grad()\n",
    "        \n",
    "        # Print the progress\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "65a7dc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 101.0640\n",
      "Epoch [20/100], Loss: 41.2558\n",
      "Epoch [30/100], Loss: 20.4440\n",
      "Epoch [40/100], Loss: 38.2093\n",
      "Epoch [50/100], Loss: 14.2622\n",
      "Epoch [60/100], Loss: 0.2922\n",
      "Epoch [70/100], Loss: 2.0149\n",
      "Epoch [80/100], Loss: 1.1263\n",
      "Epoch [90/100], Loss: 1.5358\n",
      "Epoch [100/100], Loss: 1.9914\n"
     ]
    }
   ],
   "source": [
    "model = nn.Linear(3,2)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "fit(100, model, F.mse_loss, optimizer, train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f556a4d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 58.1712,  70.5943],\n",
       "        [ 83.1211, 100.6067],\n",
       "        [120.6332, 133.9079],\n",
       "        [ 22.0033,  37.4102],\n",
       "        [102.6614, 118.7531],\n",
       "        [ 58.1712,  70.5943],\n",
       "        [ 83.1211, 100.6067],\n",
       "        [120.6332, 133.9079],\n",
       "        [ 22.0033,  37.4102],\n",
       "        [102.6614, 118.7531],\n",
       "        [ 58.1712,  70.5943],\n",
       "        [ 83.1211, 100.6067],\n",
       "        [120.6332, 133.9079],\n",
       "        [ 22.0033,  37.4102],\n",
       "        [102.6614, 118.7531]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ecd2d08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.],\n",
       "        [ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.],\n",
       "        [ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a0e375e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "440a3032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=3, out_features=2, bias=True)\n",
      "<generator object Module.parameters at 0x7f04536d50f8>\n",
      "Parameter containing:\n",
      "tensor([[-0.3934,  0.8671,  0.6582],\n",
      "        [-0.2928,  0.8117,  0.8684]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.4906, 0.2416], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "print(model.parameters())\n",
    "print(model.weight)\n",
    "\n",
    "print(model.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced6a0d9",
   "metadata": {},
   "source": [
    "### Snipperts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf7a3c6",
   "metadata": {},
   "source": [
    "###### Split data\n",
    "```python\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "train_ds, val_ds = random_split(dataset, [50000, 10000])\n",
    "len(train_ds), len(val_ds)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bf4c36",
   "metadata": {},
   "source": [
    "##### DataLoader\n",
    "We can now create data loaders to help us load the data in batches. We'll use a batch size of 128.\n",
    "```python\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size)\n",
    "```\n",
    "We set shuffle=True for the training data loader to ensure that the batches generated in each epoch are different. This randomization helps generalize & speed up the training process. On the other hand, since the validation data loader is used only for evaluating the model, there is no need to shuffle the images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6b2c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8df2ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5b4752",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1080aa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchtest",
   "language": "python",
   "name": "torchtest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
